{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial BeautifulSoup4 Demo:\n",
    "- [Site being scraped](https://realpython.github.io/fake-jobs/)\n",
    "- [Guide Site](https://realpython.com/beautiful-soup-web-scraper-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def receive_structure():\n",
    "    url = \"https://www.amazon.com/\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    with open(\"amz-home-structure.html\", \"w\") as file:\n",
    "        file.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_structure_of_application_page(data):\n",
    "    link = data[\"link\"][1] # only grab the first link because they are all the same\n",
    "    response = requests.get(link)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    with open(\"application-structure.html\", \"w\") as file:\n",
    "        file.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    url = \"https://realpython.github.io/fake-jobs/\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    collective_job_data = []\n",
    "\n",
    "    job_cards = soup.find_all(\"div\", class_=\"card-content\")\n",
    "\n",
    "    for card in job_cards:\n",
    "        job_data = {}\n",
    "        if card.find(\"time\"):\n",
    "            job_data[\"date\"] = card.find(\"time\").text.strip()\n",
    "        if card.find(\"h2\", class_=\"title\"):\n",
    "            job_data[\"title\"] = card.find(\"h2\", class_=\"title\").text.strip()\n",
    "        if card.find(\"p\", class_=\"location\"):\n",
    "            job_data[\"location\"] = card.find(\"p\", class_=\"location\").text.strip()\n",
    "        if card.find(\"h3\", class_=\"company\"):\n",
    "            job_data[\"company\"] = card.find(\"h3\", class_=\"subtitle is-6 company\").text.strip()\n",
    "        if card.find_all(\"a\")[1][\"href\"]:\n",
    "            job_data[\"link\"] = card.find_all(\"a\")[1][\"href\"]\n",
    "        collective_job_data.append(job_data)\n",
    "\n",
    "    return pd.DataFrame(collective_job_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_description_from_application_link(data):\n",
    "    for index, row in data.iterrows():\n",
    "        response = requests.get(row[\"link\"])\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        if soup.find(\"p\", class_=\"content\"):\n",
    "            data.loc[index, \"description\"] = soup.find(\"div\", class_=\"content\").text.strip()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(data):\n",
    "    data.to_csv(\"jobs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "\n",
    "receive_structure()\n",
    "extract_structure_of_application_page(data)\n",
    "\n",
    "write_to_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Senior Python Developer</td>\n",
       "      <td>Stewartbury, AA</td>\n",
       "      <td>Payne, Roberts and Davis</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Energy engineer</td>\n",
       "      <td>Christopherville, AA</td>\n",
       "      <td>Vasquez-Davidson</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Legal executive</td>\n",
       "      <td>Port Ericaburgh, AA</td>\n",
       "      <td>Jackson, Chambers and Levy</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Fitness centre manager</td>\n",
       "      <td>East Seanview, AP</td>\n",
       "      <td>Savage-Bradley</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>Product manager</td>\n",
       "      <td>North Jamieview, AP</td>\n",
       "      <td>Ramirez Inc</td>\n",
       "      <td>https://realpython.github.io/fake-jobs/jobs/pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                    title              location  \\\n",
       "0  2021-04-08  Senior Python Developer       Stewartbury, AA   \n",
       "1  2021-04-08          Energy engineer  Christopherville, AA   \n",
       "2  2021-04-08          Legal executive   Port Ericaburgh, AA   \n",
       "3  2021-04-08   Fitness centre manager     East Seanview, AP   \n",
       "4  2021-04-08          Product manager   North Jamieview, AP   \n",
       "\n",
       "                      company  \\\n",
       "0    Payne, Roberts and Davis   \n",
       "1            Vasquez-Davidson   \n",
       "2  Jackson, Chambers and Levy   \n",
       "3              Savage-Bradley   \n",
       "4                 Ramirez Inc   \n",
       "\n",
       "                                                link  \n",
       "0  https://realpython.github.io/fake-jobs/jobs/se...  \n",
       "1  https://realpython.github.io/fake-jobs/jobs/en...  \n",
       "2  https://realpython.github.io/fake-jobs/jobs/le...  \n",
       "3  https://realpython.github.io/fake-jobs/jobs/fi...  \n",
       "4  https://realpython.github.io/fake-jobs/jobs/pr...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location\n",
       "Stewartbury, AA         1\n",
       "Christopherville, AA    1\n",
       "Port Ericaburgh, AA     1\n",
       "East Seanview, AP       1\n",
       "North Jamieview, AP     1\n",
       "                       ..\n",
       "Lake Abigail, AE        1\n",
       "Jacobshire, AP          1\n",
       "Port Susan, AE          1\n",
       "North Tiffany, AA       1\n",
       "Michelleville, AP       1\n",
       "Name: count, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All locations are unique\n",
    "data[\"location\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "receive_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Amazon\n",
    "_____________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers needed so Amazon thinks we are a user and not a program\n",
    "# This is a common practice when scraping websites because some websites try to block programs from scraping\n",
    "headers = {\n",
    "    \"User-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "    \"Accept-encoding\": \"gzip, deflate, br\",\n",
    "    \"Accept-language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept\": \"image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8\",\n",
    "    \"Referer\": \"https://www.amazon.com/\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.amazon.com/Bose-QuietComfort-45-Bluetooth-Canceling-Headphones/dp/B098FKXT8L\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "soup.prettify()\n",
    "\n",
    "# Save the structure of the product page for visualization\n",
    "with open(\"amz-product-structure.html\", \"w\", encoding='utf-8') as file:\n",
    "    file.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing scraping by class and component combination\n",
    "found = soup.find(\"span\", class_=\"a-price a-text-price\")\n",
    "found\n",
    "\n",
    "with open (\"li.html\", \"w\") as file:\n",
    "    file.write(found.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking input from user then scraping page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "    \"Accept-encoding\": \"gzip, deflate, br\",\n",
    "    \"Accept-language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept\": \"image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8\",\n",
    "    \"Referer\": \"https://www.amazon.com/\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\"\n",
    "}\n",
    "\n",
    "search_query = \"bose headphones\"\n",
    "url = f\"https://www.amazon.com/s?k={search_query}\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "soup.prettify()\n",
    "\n",
    "with open(\"amz-search-structure.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = soup.find(\"div\", class_=\"s-main-slot s-result-list s-search-results sg-row\")\n",
    "\n",
    "with open(\"search-results.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(found.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = soup.find_all(\"div\", class_=\"sg-col-20-of-24 s-result-item sg-col-0-of-12 sg-col-16-of-20 s-widget sg-col sg-col-12-of-16 s-widget-spacing-large\")\n",
    "\n",
    "with open(\"search-results.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for item in found:\n",
    "        file.write(item.prettify())\n",
    "        # a-section a-spacing-small a-spacing-top-small"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
